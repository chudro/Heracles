{
  "metadata" : {
    "name" : "Stream Ratings from Kafka",
    "user_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : "/home/data/.ivy2",
    "customRepos" : null,
    "customDeps" : [ "com.datastax.spark:spark-cassandra-connector_2.10:1.4.0-M3", "org.apache.spark:spark-streaming-kafka_2.10:1.4.1", "org.apache.spark % spark-graphx_2.10 % 1.4.1", "- org.apache.spark % spark-core_2.10 % _", "- org.apache.spark % spark-streaming_2.10 % _", "- org.apache.hadoop % _ % _" ],
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : {
      "spark.cassandra.connection.host" : "127.0.0.1",
      "spark.master" : "spark://127.0.0.1:7077",
      "spark.executor.cores" : "1",
      "spark.executor.memory" : "512m",
      "spark.cores.max" : "1",
      "spark.eventLog.enabled" : "true",
      "spark.eventLog.dir" : "logs"
    }
  },
  "cells" : [ {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "# Setup (boilerplate)"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val sqlContext = new org.apache.spark.sql.SQLContext(sparkContext)\nimport sqlContext.implicits._\nimport org.apache.spark.sql.functions._\n",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@4e2f5cfa\nimport sqlContext.implicits._\nimport org.apache.spark.sql.functions._\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 1
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "## Creating the domain object"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "case class Rating(user_id: Int, movie_id: Int, rating: Float, batchtime:Long) {\n  def toCSV=s\"$user_id,$rating,$movie_id\"\n}",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "# Viz"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Reactive list of data (capped at 20 elements)"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "val list = ul(20)",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "# Spark Streaming consuming Kafka"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "The context and kafka conf"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.streaming.Seconds\nimport org.apache.spark.streaming.StreamingContext\n\nval ssc = new StreamingContext(sc, Seconds(2))\nval brokers = \"localhost:9092\"\nval topics = Set(\"ratings\")\nval kafkaParams = Map[String, String](\"metadata.broker.list\" -> brokers)",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Biz:\n* Consuming Kafka, \n* creating Ratings, \n* computing moving average\n* update list and line plot"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.streaming.kafka.KafkaUtils\nimport org.apache.spark.streaming.Time\nimport kafka.serializer.StringDecoder\nimport org.joda.time.DateTime\nimport org.apache.spark.sql.SaveMode\nimport sqlContext.implicits._\n\n val ratingsStream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, topics)\n\n    ratingsStream.foreachRDD {\n      (message: RDD[(String, String)], batchTime: Time) => {\n        // convert each RDD from the batch into a Ratings DataFrame\n        //rating data has the format user_id:movie_id:rating:timestamp\n        val df = message.map {\n          case (key, nxtRating) => nxtRating.split(\"::\")\n        }.map(rating => {\n          val timestamp: Long = new DateTime(rating(3).trim.toLong).getMillis\n          Rating(rating(0).trim.toInt, rating(1).trim.toInt, rating(2).trim.toFloat, timestamp)\n        } ).toDF(\"user_id\", \"movie_id\", \"rating\", \"timestamp\")\n\n        // this can be used to debug dataframes\n        //if (debugOutput)\n        //  df.show()\n\n        // save the DataFrame to Cassandra\n        // Note:  Cassandra has been initialized through spark-env.sh\n        //        Specifically, export SPARK_JAVA_OPTS=-Dspark.cassandra.connection.host=127.0.0.1\n        df.write.format(\"org.apache.spark.sql.cassandra\")\n          .mode(SaveMode.Append)\n          .options(Map(\"keyspace\" -> \"movie_db\", \"table\" -> \"rating_by_movie\"))\n          .save()\n        \n        list.appendAll(message.take(10).toList.map(_.toString))\n      }\n    }\n",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Start consuming"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "ssc.start()",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "## Stop all"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Stop the streaming context (keeping the spark context up, just in case)"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "//ssc.stop(false) // commented in case of a Run All ^^",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Stop the producer"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "//stopSending = true // commented in case of a Run All ^^",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "",
    "outputs" : [ ]
  } ],
  "nbformat" : 4
}
